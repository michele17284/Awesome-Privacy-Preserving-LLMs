# Awesome-Privacy-Preserving-LLMs [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
LLMs have taken the world by storm, showing outstanding capabilities in several NLP-related domains. They have been proven to have astonishing emergent capabilities and unfortunately it has become painfully obvious that memorization is one of them. While this is not a problem for models dealing with public data, when the task at hand requires to deal with sensitive data this issue cannot be overlooked. This is why, spurring from our research survey, we present here a curated list of papers on the subjects of LLMs data memorization, the privacy attacks that this allows and potential solutions, including data anonymization, Differential Privacy and Machine Unlearning.

## Table of Contents

-[Awesome-Privacy-Preserving-LLMs](#awesome-privacy-preserving-llms)
  - [Attacks] (#attacks)
  - [Data anonymization] (#data-anonymization)
  - [Pre-training with Differential Privacy] (#pretraining-with-differential-privacy)
  - [Fine-tuning with Differential Privacy] (#fine-tuning-with-differential-privacy)
  - [Parameter-Efficient Fine-Tuning with Differential Privacy] (#parameter-efficient-fine-tuning-with-differential-privacy)
  - [Reinforcement Learning with Differential Privacy] (#reinforcement-learning-with-differential-privacy)
  - [Inference with Differential Privacy] (inference-with-differential-privacy)
  - [Federated Learning with Differential Privacy] (federated-learning-with-differential-privacy)
  - [Machine Unlearning] (machine-unlearning)
  - [Tools and Frameworks] (tools-and-frameworks)
